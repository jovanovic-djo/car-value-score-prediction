{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "import joblib\n",
    "import pickle\n",
    "import os \n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    confusion_matrix, \n",
    "    classification_report\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..\\\\data\\\\clean\\\\ordinal_encoded.csv')\n",
    "\n",
    "X = df.drop('class_value', axis=1)\n",
    "y = df['class_value']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "class_names = ['Unacceptable', 'Acceptable', 'Good', 'Very Good']\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "graph_directory = \"..\\\\graphs\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameter Grids for Different Models\n",
    "model_configs = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(random_state=42),\n",
    "        'param_grid': {\n",
    "            'solver': ['lbfgs', 'liblinear'],\n",
    "            'C': [0.1, 1, 10],\n",
    "            'max_iter': [500, 1000, 1500]\n",
    "        }\n",
    "    },\n",
    "    'Naive Bayes': {\n",
    "        'model': GaussianNB(),\n",
    "        'param_grid': {\n",
    "            # Note: GaussianNB has very limited hyperparameters\n",
    "            'var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "        }\n",
    "    },\n",
    "    'SVC': {\n",
    "        'model': SVC(random_state=42),\n",
    "        'param_grid': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['rbf', 'linear'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "    },\n",
    "    'KNN': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'param_grid': {\n",
    "            'n_neighbors': [3, 5, 7],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'algorithm': ['auto', 'ball_tree', 'kd_tree']\n",
    "        }\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'model': DecisionTreeClassifier(random_state=42),\n",
    "        'param_grid': {\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_metrics(model, X_train, X_test, y_train, y_test, model_name, class_names):\n",
    "    \"\"\"\n",
    "    Evaluate a model using various classification metrics\n",
    "    \"\"\"\n",
    "    # Train the model and make predictions\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'model_name': model_name,\n",
    "        'training_time': training_time,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, average='macro', zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, average='macro', zero_division=0),\n",
    "        'f1_score': f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Model Performance:\")\n",
    "    for metric, value in metrics.items():\n",
    "        if metric not in ['model_name']:\n",
    "            if metric == 'training_time':\n",
    "                print(f\"{metric.capitalize()}: {value:.2f} seconds\")\n",
    "            else:\n",
    "                print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=class_names, zero_division=0))\n",
    "    \n",
    "    return metrics, y_pred\n",
    "\n",
    "def plot_confusion_matrix(y_test, y_pred, model_name, class_names, graph_directory):\n",
    "    \"\"\"\n",
    "    Plot and save confusion matrix\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues', \n",
    "        xticklabels=class_names, \n",
    "        yticklabels=class_names\n",
    "    )\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{graph_directory}confusion_matrices\\\\{model_name.lower()} confusion matrix\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importance(model, model_name, feature_names, graph_directory):\n",
    "    \"\"\"\n",
    "    Plot and save feature importance if the model supports it\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if hasattr(model, 'coef_'):\n",
    "            feature_importance = np.abs(model.coef_[0])\n",
    "        elif hasattr(model, 'feature_importances_'):\n",
    "            feature_importance = model.feature_importances_\n",
    "        else:\n",
    "            print(f\"Model {model_name} doesn't support feature importance visualization\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': feature_importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        sns.barplot(x='importance', y='feature', data=feature_importance_df)\n",
    "        plt.title(f'{model_name} - Feature Importance')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.ylabel('Features')\n",
    "        plt.tight_layout()\n",
    "        #plt.savefig(f\"{graph_directory}feature_importance\\\\{model_name.lower()} feature importance\")\n",
    "        plt.show()\n",
    "        \n",
    "        return feature_importance_df\n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot feature importance: {e}\")\n",
    "        return None\n",
    "\n",
    "def optimize_model(model, param_grid, X_train, y_train, cv_splits=5):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning using GridSearchCV\n",
    "    \"\"\"\n",
    "    cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    optimization_time = time.time() - start_time\n",
    "    \n",
    "    print(\"\\nBest parameters:\", grid_search.best_params_)\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"Optimization time: {optimization_time:.2f} seconds\")\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "def evaluate_all_models(model_configs, X_train, X_test, y_train, y_test, class_names, feature_names, graph_directory):\n",
    "    \"\"\"\n",
    "    Evaluate all models and return their metrics\n",
    "    \"\"\"\n",
    "    all_metrics = []\n",
    "    \n",
    "    for model_name, config in model_configs.items():\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        \n",
    "        # Get the model from config\n",
    "        model = config['model']\n",
    "        \n",
    "        # Evaluate model\n",
    "        metrics, y_pred = evaluate_model_metrics(\n",
    "            model, X_train, X_test, y_train, y_test, model_name, class_names\n",
    "        )\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plot_confusion_matrix(\n",
    "            y_test, y_pred, model_name, class_names, graph_directory\n",
    "        )\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plot_feature_importance(\n",
    "            model, model_name, feature_names, graph_directory\n",
    "        )\n",
    "        \n",
    "        all_metrics.append(metrics)\n",
    "    \n",
    "    # Create DataFrame with all metrics\n",
    "    metrics_df = pd.DataFrame(all_metrics)\n",
    "    metrics_df.set_index('model_name', inplace=True)\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "def optimize_best_model(best_model_name, model_configs, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Optimize the best performing model using its predefined parameter grid\n",
    "    \"\"\"\n",
    "    if best_model_name not in model_configs:\n",
    "        raise ValueError(f\"Model {best_model_name} not found in configurations\")\n",
    "    \n",
    "    config = model_configs[best_model_name]\n",
    "    model = config['model']\n",
    "    param_grid = config['param_grid']\n",
    "    \n",
    "    print(f\"\\nOptimizing {best_model_name}...\")\n",
    "    return optimize_model(model, param_grid, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Evaluate all models first\n",
    "metrics_df = evaluate_all_models(\n",
    "    model_configs,\n",
    "    X_train_scaled,\n",
    "    X_test_scaled,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    class_names,\n",
    "    feature_names,\n",
    "    graph_directory\n",
    ")\n",
    "\n",
    "# 2. Find the best model based on F1 score\n",
    "best_model_name = metrics_df['f1_score'].idxmax()\n",
    "print(f\"\\nBest performing model: {best_model_name}\")\n",
    "print(\"\\nAll models performance summary:\")\n",
    "print(metrics_df)\n",
    "\n",
    "# 3. Optimize the best model\n",
    "best_model, best_params, best_score = optimize_best_model(\n",
    "    best_model_name,\n",
    "    model_configs,\n",
    "    X_train_scaled,\n",
    "    y_train\n",
    ")\n",
    "\n",
    "# 4. Evaluate the optimized model\n",
    "final_metrics, final_predictions = evaluate_model_metrics(\n",
    "    best_model,\n",
    "    X_train_scaled,\n",
    "    X_test_scaled,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    f\"Optimized {best_model_name}\",\n",
    "    class_names\n",
    ")\n",
    "\n",
    "# 5. Plot final confusion matrix and feature importance\n",
    "plot_confusion_matrix(\n",
    "    y_test,\n",
    "    final_predictions,\n",
    "    f\"Optimized {best_model_name}\",\n",
    "    class_names,\n",
    "    graph_directory\n",
    ")\n",
    "\n",
    "plot_feature_importance(\n",
    "    best_model,\n",
    "    f\"Optimized {best_model_name}\",\n",
    "    feature_names,\n",
    "    graph_directory\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
